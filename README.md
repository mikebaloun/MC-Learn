# MC-Learn: Accelerating Transformer Training with Monte Carlo Sampling

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mikebaloun/MC-Learn/blob/main/Monte_Carlo_Learn.ipynb)

MC-Learn is an adaptive sampling algorithm designed to reduce the wall-clock time and computational cost of fine-tuning Transformer models.  
By selectively training on the **most informative examples**, it achieves near-baseline performance in a fraction of the time.

---

## ğŸš© Motivation

Training large language models is expensive.  
Standard fine-tuning wastes resources by treating **every example equally** â€” spending the same compute on easy, redundant data as on difficult, high-variance samples.

MC-Learn changes this by **learning where to spend compute**.

---

## ğŸ§© How It Works

Instead of processing the full dataset every step:

1. **Score:** A lightweight surrogate head scores a candidate pool for difficulty.  
2. **Select:** A smaller, high-value batch is drawn (with replay regularization).  
3. **Train:** The model updates on this batch with a near-unbiased, low-variance gradient estimate.  
4. **Adapt:** Sampling policy and budget update dynamically based on runtime measurements and variance signals.

This creates a **compute-efficient training loop**: spend more on the hardest, most useful data; spend less on redundant data.

---

## âš™ï¸ System Components

### ğŸ§  Policy Learner
Learns a sampling strategy online:
- **Class Weights:** Allocate more budget to classes with higher gradient variance.  
- **Sampling Temperature (Ï„):** Control sharpness of within-class selection.  
- **Replay Share (Ï):** Mix in a replay buffer to regularize and prevent forgetting.  
- **ESS Guard:** Monitors effective sample size to keep importance weighting stable.

### âš–ï¸ Budget Solver
Allocates compute under a fixed budget:
- Tracks **measured per-example costs** of scoring, cheap head, and full forward.  
- Solves for optimal batch size *M* and inclusion probabilities *q*.  
- Ensures wall-clock training stays within target budget ratio.

### ğŸ‘· Sampler
Builds the actual training batch:
- Scores a candidate pool with the surrogate head.  
- Uses **soft top-k + replay union** to keep both hard and easy examples.  
- Computes clipped **importance weights** for unbiasedness and variance control.

### ğŸ”„ Trainer
Runs the training loop with a **control variate estimator**:

\[
\hat{L} = \sum_i w_i \, \ell_h(i) \;+\; \sum_{i \in S} \frac{w_i}{q_i}\,\big(\ell_f(i) - \ell_h(i)\big)
\]

- \( \ell_h \): cheap surrogate loss  
- \( \ell_f \): full forward loss  
- \( w_i \): importance weights  
- \( q_i \): inclusion probability for full forward  

This estimator reduces variance while remaining nearly unbiased.

---

## ğŸ“Š Results

### Experimental Setup
- **Model:** `distilbert-base-uncased` (4-class classifier head)  
- **Dataset:** AG News (120k training samples)  
- **Task:** Topic classification (World, Sports, Business, Sci/Tech)  
- **Environment:** Google Colab T4 GPU  
- **Runs:** 1 epoch equivalent, averaged over 3 seeds (42/43/44)  

### Performance

| Run      | Accuracy (Mean Â± Std)   | Time (s) (Mean Â± Std) | Speedup |
|----------|-------------------------|-----------------------|---------|
| Baseline | 0.9309 Â± 0.0004         | 316.3 Â± 16.4          | 1.00Ã—   |
| MC-Learn | 0.8698 Â± 0.0008         | 111.7 Â± 2.3           | 2.83Ã—   |

**Takeaway:** MC-Learn trains **~2.8Ã— faster** with only a **6.1% absolute accuracy drop**.  
This is ideal for rapid prototyping and compute-constrained settings.

*Note: Comparison is compute-matched (wall-clock), not loss-matched. MC-Learn uses EMA distillation for stability.*

---

## ğŸ” Diagnostics

MC-Learn includes built-in analysis tools:
- **Confusion Matrix**: visualize which classes are confused.  
- **Difficulty Score Distributions**: detect mismatches between scorer and true difficulty (â€œsmoking gunâ€ analysis).  

These help identify when the surrogate scorer is under- or over-confident.

---

## ğŸ“š Background

MC-Learn builds on established concepts:
- **Importance Sampling & Control Variates** â€” variance reduction in Monte Carlo estimators.  
- **Curriculum & Active Learning** â€” focusing compute on harder or more informative examples.  
- **Efficient Transformer Training** â€” complements pruning, quantization, and distillation.  

---

## ğŸš€ How to Run

1. Click the **Colab badge** at the top.  
2. Enable GPU: **Runtime â†’ Change runtime type â†’ GPU**.  
3. Run all cells.  
4. The script will:
   - Train a baseline run  
   - Train MC-Learn with adaptive sampling  
   - Output metrics and diagnostics  

---

## ğŸ“Œ Notes
- Automatically adapts presets for fast GPUs (e.g. A100 with bf16).  
- Importance weights are clipped at `w_clip = 10.0` to ensure stability.  
- Designed for reproducibility: controlled seeds and fixed evaluation protocol.  

---
